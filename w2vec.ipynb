{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from FULL_DATA.ipynb\n",
      "importing Jupyter notebook from Create_Labels.ipynb\n",
      "(6380, 4)\n",
      "4400\n",
      "4467\n",
      "4377\n",
      "(4400, 7)\n",
      "(4377, 7)\n",
      "(4437, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\grays\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import matplotlib.pyplot as plt\n",
    "from FULL_DATA import final_df\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    2367\n",
      " 0    1144\n",
      "-1     866\n",
      "Name: SENTIMENT, dtype: int64\n",
      "(4377, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRANSCRIPTS</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>TRANSCRIPT_DATE</th>\n",
       "      <th>RATINGS_DATE</th>\n",
       "      <th>ratings</th>\n",
       "      <th>DATE_DIFF</th>\n",
       "      <th>KEY</th>\n",
       "      <th>SENTIMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14598</th>\n",
       "      <td>['Image source: The Motley Fool.', 'Agilent Te...</td>\n",
       "      <td>A</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>2018-09-28</td>\n",
       "      <td>4.714</td>\n",
       "      <td>59 days</td>\n",
       "      <td>A_2018-09-28</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14595</th>\n",
       "      <td>['Image source: The Motley Fool.', 'Agilent Te...</td>\n",
       "      <td>A</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>4.571</td>\n",
       "      <td>42 days</td>\n",
       "      <td>A_2018-12-31</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14588</th>\n",
       "      <td>['Image source: The Motley Fool.', 'Agilent Te...</td>\n",
       "      <td>A</td>\n",
       "      <td>2019-01-31</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>5.000</td>\n",
       "      <td>57 days</td>\n",
       "      <td>A_2019-03-29</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14585</th>\n",
       "      <td>['Image source: The Motley Fool.', 'Agilent Te...</td>\n",
       "      <td>A</td>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>4.867</td>\n",
       "      <td>45 days</td>\n",
       "      <td>A_2019-06-28</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14582</th>\n",
       "      <td>['Image source: The Motley Fool.', 'Agilent Te...</td>\n",
       "      <td>A</td>\n",
       "      <td>2019-08-14</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>4.286</td>\n",
       "      <td>47 days</td>\n",
       "      <td>A_2019-09-30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             TRANSCRIPTS TICKER  \\\n",
       "14598  ['Image source: The Motley Fool.', 'Agilent Te...      A   \n",
       "14595  ['Image source: The Motley Fool.', 'Agilent Te...      A   \n",
       "14588  ['Image source: The Motley Fool.', 'Agilent Te...      A   \n",
       "14585  ['Image source: The Motley Fool.', 'Agilent Te...      A   \n",
       "14582  ['Image source: The Motley Fool.', 'Agilent Te...      A   \n",
       "\n",
       "      TRANSCRIPT_DATE RATINGS_DATE  ratings DATE_DIFF           KEY  SENTIMENT  \n",
       "14598      2018-07-31   2018-09-28    4.714   59 days  A_2018-09-28         -1  \n",
       "14595      2018-11-19   2018-12-31    4.571   42 days  A_2018-12-31         -1  \n",
       "14588      2019-01-31   2019-03-29    5.000   57 days  A_2019-03-29         -1  \n",
       "14585      2019-05-14   2019-06-28    4.867   45 days  A_2019-06-28         -1  \n",
       "14582      2019-08-14   2019-09-30    4.286   47 days  A_2019-09-30          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# threshold = .1\n",
    "# final_df['NEXT_RATING'] = final_df.groupby('TICKER')['ratings'].shift(1)\n",
    "# final_df['Rating_Change'] = (final_df['ratings']-final_df['NEXT_RATING']) \n",
    "# # final_df['%_Delta'] = (final_df['Rating_Change']/final_df['NEXT_RATING']) \n",
    "# final_df.dropna(inplace = True)\n",
    "# print(final_df.shape[0])\n",
    "# final_df['SENTIMENT'] = [1 if x >= threshold else -1 if  x<=-threshold else 0 for x in final_df['Rating_Change']]\n",
    "# final_df.reset_index(inplace = True)\n",
    "\n",
    "\n",
    "# final_df['NEXT_RATING'] = final_df.groupby('TICKER')['ratings'].shift(1)\n",
    "# final_df['Rating_Change'] = (final_df['ratings']-final_df['NEXT_RATING']) \n",
    "# final_df.dropna(inplace = True)\n",
    "# print(final_df.shape[0])\n",
    "# final_df['SENTIMENT'] = [1 if x > 0 else 0 if x ==0 else 2 for x in final_df['Rating_Change']]\n",
    "# final_df.head(20)\n",
    "\n",
    "\n",
    "\n",
    "final_df['SENTIMENT'] = [0 if (x > 1 and x<3.4999) else 1 if (x > 3.5 and x<4.49999) else -1  for x in final_df['ratings']]\n",
    "print(final_df['SENTIMENT'].value_counts())\n",
    "print(final_df.shape)\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_words = []\n",
    "# # nltk.download('wordnet')\n",
    "# from nltk.stem import PorterStemmer \n",
    "# ps = PorterStemmer()\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "# lemmatizer = WordNetLemmatizer() \n",
    "# counter = 0\n",
    "# for i in final_df['TRANSCRIPTS']: \n",
    "#     print(\"counter\",counter)\n",
    "#     counter+=1\n",
    "#     for j in word_tokenize(i.lower()):\n",
    "#         lem_word = lemmatizer.lemmatize(j)\n",
    "#         stem_word = ps.stem(lem_word)\n",
    "#         if stem_word in stopwords.words('english'):\n",
    "#             pass\n",
    "#         else:\n",
    "#             all_words.append(stem_word)\n",
    "# all_words            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['SENTIMENT'].value_counts()\n",
    "# len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each sentence in the file \n",
    "data = [] \n",
    "for i in final_df['TRANSCRIPTS']: \n",
    "    temp = [] \n",
    "    # tokenize the sentence into words \n",
    "#     print(i)\n",
    "    for j in word_tokenize(i): \n",
    "        if j in temp:\n",
    "            pass\n",
    "        elif j in stopwords.words('english'):\n",
    "            pass\n",
    "        else:\n",
    "            temp.append(j.lower()) \n",
    "    data.append(temp)         \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer(lowercase=True,max_df = .9,min_df=.1,ngram_range = (1,1))\n",
    "text_tf= tf.fit_transform(final_df['TRANSCRIPTS'])\n",
    "tfidf = dict(zip(tf.get_feature_names(), tf.idf_))\n",
    "# tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_article = article_text.lower()\n",
    "# processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "# processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "# final_df['TRANSCRIPTS'] = final_df['TRANSCRIPTS'].apply(word_tokenize)\n",
    "\n",
    "max_len=20\n",
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec(data, min_count=2,size = max_len, window = 5)\n",
    "vocabulary = word2vec.wv.vocab\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_list = []   \n",
    "import numpy as np\n",
    "for i in final_df['TRANSCRIPTS']: \n",
    "    vec = np.zeros(max_len).reshape((1, max_len))\n",
    "    count = 0\n",
    "#     print(\"iiiiiiiiiiiiiiiiiiiiii\",i)\n",
    "    for j in word_tokenize(i):\n",
    "#         print(j)\n",
    "        try:\n",
    "            vec += word2vec[j].reshape((1, max_len)) * tfidf[j]\n",
    "            count += 1.\n",
    "        except KeyError: \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    avg_list.append(vec[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# print(avg_list[0:2])\n",
    "pd.DataFrame(avg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec.similarity('computer', 'phone')\n",
    "word2vec.wv.most_similar('shares')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     pd.DataFrame(avg_list), final_df['SENTIMENT'], test_size=0.15, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pd.DataFrame(avg_list), final_df['Rating_Change'], test_size=0.15, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=1, multi_class= 'multinomial',solver = 'lbfgs',max_iter=200).fit(X_train, y_train)\n",
    "print(\"Train Logistic Accuracy:\",metrics.accuracy_score(y_train, clf.predict(X_train)))\n",
    "print(\"Test Logistic Accuracy:\",metrics.accuracy_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=6, random_state=1,n_estimators=10000)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Train RF Accuracy:\",metrics.accuracy_score(y_train, clf.predict(X_train)))\n",
    "print(\"Test RF Accuracy:\",metrics.accuracy_score(y_test, clf.predict(X_test)))\n",
    "# clf.predict_proba(X_test)\n",
    "print(\"Confusion Matrix RF:\",metrics.multilabel_confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "epochs = 20000\n",
    "lr =.1\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='sigmoid', input_dim=max_len))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('linear'))\n",
    "sgd = SGD(lr=0.05, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mae', optimizer=sgd)\n",
    "# early = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=.0001, patience=50, verbose=2, mode='auto', baseline=None, restore_best_weights=True)\n",
    "model.fit(np.array(X_train), np.array(y_train), epochs=epochs, batch_size=50, verbose=2,validation_data=(np.array(X_test), np.array(y_test)), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=128, verbose=2)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
