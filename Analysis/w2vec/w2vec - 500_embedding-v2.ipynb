{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from FULL_DATA.ipynb\n",
      "importing Jupyter notebook from Create_Labels.ipynb\n",
      "(6383, 4)\n",
      "4402\n",
      "(4402, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Chad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import matplotlib.pyplot as plt\n",
    "from FULL_DATA import final_df\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    2367\n",
      " 0    1144\n",
      "-1     866\n",
      "Name: SENTIMENT, dtype: int64\n",
      "(4377, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRANSCRIPTS</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>TRANSCRIPT_DATE</th>\n",
       "      <th>RATINGS_DATE</th>\n",
       "      <th>ratings</th>\n",
       "      <th>DATE_DIFF</th>\n",
       "      <th>KEY</th>\n",
       "      <th>SENTIMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14602</th>\n",
       "      <td>['Image source: The Motley Fool.', 'Agilent Te...</td>\n",
       "      <td>A</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>2018-09-28</td>\n",
       "      <td>4.714</td>\n",
       "      <td>59 days</td>\n",
       "      <td>A_2018-09-28</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14599</th>\n",
       "      <td>['Image source: The Motley Fool.', 'Agilent Te...</td>\n",
       "      <td>A</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>4.571</td>\n",
       "      <td>42 days</td>\n",
       "      <td>A_2018-12-31</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14592</th>\n",
       "      <td>['Image source: The Motley Fool.', 'Agilent Te...</td>\n",
       "      <td>A</td>\n",
       "      <td>2019-01-31</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>5.000</td>\n",
       "      <td>57 days</td>\n",
       "      <td>A_2019-03-29</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14589</th>\n",
       "      <td>['Image source: The Motley Fool.', 'Agilent Te...</td>\n",
       "      <td>A</td>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>4.867</td>\n",
       "      <td>45 days</td>\n",
       "      <td>A_2019-06-28</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14586</th>\n",
       "      <td>['Image source: The Motley Fool.', 'Agilent Te...</td>\n",
       "      <td>A</td>\n",
       "      <td>2019-08-14</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>4.286</td>\n",
       "      <td>47 days</td>\n",
       "      <td>A_2019-09-30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             TRANSCRIPTS TICKER  \\\n",
       "14602  ['Image source: The Motley Fool.', 'Agilent Te...      A   \n",
       "14599  ['Image source: The Motley Fool.', 'Agilent Te...      A   \n",
       "14592  ['Image source: The Motley Fool.', 'Agilent Te...      A   \n",
       "14589  ['Image source: The Motley Fool.', 'Agilent Te...      A   \n",
       "14586  ['Image source: The Motley Fool.', 'Agilent Te...      A   \n",
       "\n",
       "      TRANSCRIPT_DATE RATINGS_DATE  ratings DATE_DIFF           KEY  SENTIMENT  \n",
       "14602      2018-07-31   2018-09-28    4.714   59 days  A_2018-09-28         -1  \n",
       "14599      2018-11-19   2018-12-31    4.571   42 days  A_2018-12-31         -1  \n",
       "14592      2019-01-31   2019-03-29    5.000   57 days  A_2019-03-29         -1  \n",
       "14589      2019-05-14   2019-06-28    4.867   45 days  A_2019-06-28         -1  \n",
       "14586      2019-08-14   2019-09-30    4.286   47 days  A_2019-09-30          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making Labels\n",
    "final_df['SENTIMENT'] = [0 if (x > 1 and x<3.4999) else 1 if (x > 3.5 and x<4.49999) else -1  for x in final_df['ratings']]\n",
    "print(final_df['SENTIMENT'].value_counts())\n",
    "print(final_df.shape)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    2367\n",
       " 0    1144\n",
       "-1     866\n",
       "Name: SENTIMENT, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['SENTIMENT'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each sentence in the file \n",
    "data = [] \n",
    "for i in final_df['TRANSCRIPTS']: \n",
    "    temp = [] \n",
    "    # tokenize the sentence into words \n",
    "#     print(i)\n",
    "    for j in word_tokenize(i): \n",
    "        if j in temp:\n",
    "            pass\n",
    "        elif j in stopwords.words('english'):\n",
    "            pass\n",
    "        else:\n",
    "            temp.append(j.lower()) \n",
    "    data.append(temp)         \n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer(lowercase=True,max_df = .9,min_df=.1,ngram_range = (1,1))\n",
    "text_tf= tf.fit_transform(final_df['TRANSCRIPTS'])\n",
    "tfidf = dict(zip(tf.get_feature_names(), tf.idf_))\n",
    "# tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec\n",
    "max_len = 500\n",
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec(data, min_count=2,size = max_len, window = 5)\n",
    "vocabulary = word2vec.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chad\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "avg_list = []   \n",
    "import numpy as np\n",
    "for i in final_df['TRANSCRIPTS']: \n",
    "    vec = np.zeros(max_len).reshape((1, max_len))\n",
    "    count = 0\n",
    "#     print(\"iiiiiiiiiiiiiiiiiiiiii\",i)\n",
    "    for j in word_tokenize(i):\n",
    "#         print(j)\n",
    "        try:\n",
    "            vec += word2vec[j].reshape((1, max_len)) * tfidf[j]\n",
    "            count += 1.\n",
    "        except KeyError: \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    avg_list.append(vec[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     pd.DataFrame(avg_list), final_df['SENTIMENT'], test_size=0.15, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "num_classes = 3\n",
    "\n",
    "y_train_adjusted = to_categorical(np.array(y_train), num_classes = num_classes)\n",
    "y_test_adjusted = to_categorical(np.array(y_test), num_classes = num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Chad\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Chad\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28a405ede10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Simple Feed-Forward\n",
    "import tensorflow \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "epochs = 1500\n",
    "#lr =.1\n",
    "num_classes = 3\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='sigmoid', input_dim=max_len))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "#sgd = SGD(lr=0.05, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['acc'])\n",
    "model.fit(np.array(X_train), y_train_adjusted, epochs=epochs, batch_size=32, verbose=0,validation_data=(np.array(X_test), y_test_adjusted), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 3.3570 - acc: 0.6986\n",
      "Making Sure Accuracy is the same: 0.6986301369863014\n",
      "Feed-Foward Precision: 0.6933117021780927\n",
      "Feed-Foward Recall: 0.6986301369863014\n",
      "Feed-Foward F1: 0.695207910276481\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test_adjusted, batch_size=32, verbose=2)\n",
    "\n",
    "y_prob = model.predict(X_test)\n",
    "predicted = np.argmax(y_prob, axis = 1)\n",
    "\n",
    "sklearn_y_test = np.argmax(y_test_adjusted, axis = 1)\n",
    "\n",
    "print(\"Making Sure Accuracy is the same:\",metrics.accuracy_score(sklearn_y_test, predicted))\n",
    "print(\"Feed-Foward Precision:\",metrics.precision_score(sklearn_y_test, predicted, average = 'weighted'))\n",
    "print(\"Feed-Foward Recall:\",metrics.recall_score(sklearn_y_test, predicted, average = 'weighted'))\n",
    "print(\"Feed-Foward F1:\",metrics.f1_score(sklearn_y_test, predicted, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy 0.5601217656012176\n",
      "Baseline Precision: 0.31373639230022543\n",
      "Baseline Recall: 0.5601217656012176\n",
      "Baseline F1: 0.40219475071463046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chad\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#For Report - Baseline\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_df['TRANSCRIPTS'], final_df['SENTIMENT'], test_size=0.15, random_state=1)\n",
    "\n",
    "print(y_train)\n",
    "print()\n",
    "'''\n",
    "                                                            \n",
    "predicted_baseline = np.full(y_test.size, 1)\n",
    "\n",
    "print(\"Baseline Accuracy\",metrics.accuracy_score(y_test, predicted_baseline))\n",
    "print(\"Baseline Precision:\",metrics.precision_score(y_test, predicted_baseline, average = 'weighted'))\n",
    "print(\"Baseline Recall:\",metrics.recall_score(y_test, predicted_baseline, average = 'weighted'))\n",
    "print(\"Baseline F1:\",metrics.f1_score(y_test, predicted_baseline, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix DL:\n",
      "[[[436  59]\n",
      "  [ 56 106]]\n",
      "\n",
      " [[199  90]\n",
      "  [ 75 293]]\n",
      "\n",
      " [[481  49]\n",
      "  [ 67  60]]]\n"
     ]
    }
   ],
   "source": [
    "#Printing Confusion Matrix - For Report\n",
    "print(\"Confusion Matrix DL:\")\n",
    "print(metrics.multilabel_confusion_matrix(sklearn_y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
